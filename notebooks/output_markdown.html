<!--@nested-tags:entity resolution,article-->

<p><img alt="Active Learning for Large-Scale Entity Resolution" src="papers/active_learning_large_scale_ER.pdf" /></p>
<ul>
<li><a href="#sujet-de-larticle">Sujet de l'article</a></li>
<li><a href="#contexte">Contexte</a></li>
<li><a href="#objectif-principal">Objectif principal</a></li>
<li><a href="#objectifs-secondaires">Objectifs secondaires</a></li>
<li><a href="#contenu-de-larticle">Contenu de l'article</a></li>
<li><a href="#résumé">Résumé</a></li>
<li><a href="#méthodesmodélisation">Méthodes/modélisation</a></li>
<li><a href="#données">Données</a></li>
<li><a href="#résultats">Résultats</a></li>
<li><a href="#commentaires">Commentaires</a></li>
<li><a href="#remarques">Remarques</a></li>
<li><a href="#a-investiguer">A investiguer</a></li>
<li><a href="#article">Article</a></li>
</ul>
<h1>Sujet de l'article</h1>
<h2>Contexte</h2>
<ul>
<li>Previous work has focused on learning conjunction of predicates under precision guarantees, less successful at learning multiple rules sufficiently different from each other</li>
<li>Difficulties to deal with ER at scale</li>
</ul>
<h2>Objectif principal</h2>
<p><strong>Deal with ER at scale with low budget</strong></p>
<h2>Objectifs secondaires</h2>
<ul>
<li>identifying, out of a potentially massive set, a small subset of informative examples to be labeled by the user</li>
<li>using the labeled examples to efficiently learn ER algorithms that achieve both high precision and high recall</li>
<li>executing the learned algorithm to determine duplicates at scale</li>
</ul>
<h1>Contenu de l'article</h1>
<h2>Résumé</h2>
<ol>
<li>Introduction</li>
<li>Popular task that require extensive exploration and domain knowledge</li>
<li>Cross product, prohibitive at large scale</li>
<li>Strong history of supervised learning, but hard to generate a labelled set</li>
<li>Difficulties to find False Negatives</li>
<li>Built on top of HIL (Custom IBM high level language for entity management)</li>
<li>Contributions :<ul>
<li>identify role of false negative and false positives in learning task</li>
<li>high quality ER on big data</li>
<li>maximizing recall while satisfying high precision</li>
</ul>
</li>
<li>Problem definition</li>
<li>Mixed of blocking and matching predicate</li>
<li>Blocking is extracted from overall rule R</li>
<li>Objective : Learn disjunction of conjunction</li>
<li>To evaluate performance of rule R :<ul>
<li>Precision is evaluated on a subset of links</li>
<li>Links selected with low confidence to estimate precision (likely to be superior on the entire set)</li>
<li>Under high precision, number of links is a good estimate for recall</li>
</ul>
</li>
<li>Learning System</li>
<li>Learning process in 2 phases :<ul>
<li>Learn a candidate rule and generate examples to label</li>
<li>labelling, label set update, candidate rule accepted or rejected</li>
</ul>
</li>
<li>Rule only accepted if $\tau$ fraction of labels are matches, ensure high precision rule</li>
<li>Then already "blocked" links are no longer considered (removed from T - labeled set)</li>
<li>Estimate confidence with the percentage of predicates satisfied =&gt; allow for looking at false positive examples</li>
<li>search for false negatives : by generalizing rules</li>
<li>Conclusion</li>
<li>Point 1</li>
<li>Point 2</li>
</ol>
<h2>Méthodes/modélisation</h2>
<ul>
<li>
<p>Global model :
<img alt="" src="assets/al_model_large_scale.png" /></p>
</li>
<li>
<p>Single rule learning
<img alt="" src="assets/learn_rule_al_scale.png" /></p>
<blockquote>
<p>${F}_{i}$ : one feature resulting from one predicate
covg : coverage = recall
Generalize : Prune predicate in R</p>
</blockquote>
</li>
</ul>
<p>Only keep rule that contains a blocking predicate and that satisfy a precision threshold</p>
<h2>Données</h2>
<ul>
<li>DBLP - scholar</li>
<li>EMP- SOCIAL</li>
<li>CRYSTAL</li>
</ul>
<h2>Résultats</h2>
<p>à lire</p>
<h1>Commentaires</h1>
<h2>Remarques</h2>
<ul>
<li>Take both blocking and matching as a single learned rule</li>
</ul>
<h2>A investiguer</h2>
<h2>Article</h2>
<ul>
<li>On Active Learning of Record Matching Packages, 2010</li>
</ul>